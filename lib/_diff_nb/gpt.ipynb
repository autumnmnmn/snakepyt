{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "138628a8-fd16-40e4-9ae5-e8da5778d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following Andrej Karpathy's \"Let's reproduce GPT-2 (124M)\"\n",
    "# https://www.youtube.com/watch?v=l8pRSuU81PU\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bbb816a-8f0a-4e96-951b-4598c854a4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 256\n",
    "    vocab_size: int = 65\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 6\n",
    "    n_embed: int = 384\n",
    "\n",
    "@dataclass\n",
    "class GPT2_124M_Config:\n",
    "    block_size = 1024\n",
    "    vocab_size = 50257 # 50k BPE merges, 256 bytes tokens, EOT token\n",
    "    n_layer = 12\n",
    "    n_head = 12\n",
    "    n_embed = 768\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embed & config.n_head == 0\n",
    "    \n",
    "        self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed) # k, q, v projections concatenated\n",
    "        self.c_proj = nn.Linear(config.n_embed, config.n_embed)\n",
    "        self.config = config\n",
    "        self.c_proj.residual_rescale = True\n",
    "\n",
    "        # mask, lower triangular, wrapped in two singleton dimensions\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, seq length, n_embed\n",
    "\n",
    "        # nh = num heads\n",
    "        # hs = head size\n",
    "        # n_embed = nh * hs\n",
    "        q, k, v = self.c_attn(x).split(self.config.n_embed, dim=2)\n",
    "        q = q.view(B, T, self.config.n_head, C // self.config.n_head).transpose(1, 2) # B, nh, T, hs\n",
    "        k = k.view(B, T, self.config.n_head, C // self.config.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.config.n_head, C // self.config.n_head).transpose(1, 2)\n",
    "\n",
    "        #att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        #att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float(\"-inf\"))\n",
    "        #att = func.softmax(att, dim=-1)\n",
    "        #y = att @ v # (B, hn, T, T) x (B, nh, T, hs) = (B, nh, T, hs)\n",
    "        y = func.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.c_proj(y)\n",
    "        \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embed, 4 * config.n_embed)\n",
    "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
    "        self.c_proj = nn.Linear(4 * config.n_embed, config.n_embed)\n",
    "        self.c_proj.residual_rescale = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embed)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embed)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x += self.attn(self.ln_1(x))\n",
    "        x += self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embed), # Token embedding\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embed), # Positional embedding\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embed),\n",
    "        ))\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embed, config.vocab_size, bias=False)\n",
    "\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, \"residual_rescale\"):\n",
    "                std = (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device):\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
    "            {\"params\": nodecay_params, \"weight_decay\": 0.0}\n",
    "        ]\n",
    "        use_fused = \"cuda\" in device\n",
    "        return torch.optim.AdamW(optim_groups, lr=3e-4, betas=(0.9,0.95), eps=1e-8, fused=use_fused)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, \"seq len limit\"\n",
    "\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = func.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        assert model_type in {\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "\n",
    "        config_args = {\n",
    "            \"gpt2\": dict(n_layer=12, n_head=12, n_embed=768), # 124M\n",
    "            \"gpt2-medium\": dict(n_layer=24, n_head=16, n_embed=1024), # 350M\n",
    "            \"gpt2-large\": dict(n_layer=36, n_head=20, n_embed=1280), # 774M\n",
    "            \"gpt2-xl\": dict(n_layer=48, n_head=25, n_embed=1600), # 1.558B\n",
    "        }[model_type]\n",
    "\n",
    "        config_args[\"vocab_size\"] = 50257\n",
    "        config_args[\"block_size\"] = 1024\n",
    "\n",
    "        config = GPTConfig(**config_args)\n",
    "\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith(\".attn.bias\")]\n",
    "\n",
    "        hf_model = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        hf_sd = hf_model.state_dict()\n",
    "\n",
    "        hf_sd_keys = hf_sd.keys()\n",
    "        hf_sd_keys = [k for k in hf_sd_keys if not k.endswith(\".attn.masked_bias\")]\n",
    "        hf_sd_keys = [k for k in hf_sd_keys if not k.endswith(\".attn.bias\")]\n",
    "        transposed = [\"attn.c_attn.weight\", \"attn.c_proj.weight\", \"mlp.c_fc.weight\", \"mlp.c_proj.weight\"]\n",
    "\n",
    "        assert len(sd_keys) == len(hf_sd_keys), \"mismatched keys\"\n",
    "\n",
    "        for k in hf_sd_keys:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                assert hf_sd[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(hf_sd[k].t())\n",
    "            else:\n",
    "                assert hf_sd[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(hf_sd[k])\n",
    "\n",
    "        return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ac57727-6891-41f5-afc4-b04cc5ea5b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT.from_pretrained(\"gpt2\")\n",
    "#print(\"\\n\".join(model.state_dict().keys()))\n",
    "\n",
    "#model = GPT(GPT2_124M_Config())\n",
    "model.eval()\n",
    "model.to(\"cuda\")\n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fb78b34d-9d75-4153-b81d-5620fce4cd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "glurp.org and are available on the web at http://www.youtube.com/user/DrACnix\n",
      "\n",
      "The E3 2016 is happening in October, so if you love seeing games, you have more time than others to see some amazing games. The game reveal is getting underway in the second half of October.\n",
      "\n",
      "We always look forward to your participation in our E3. And the winners of the E3 should sign our petition to make their games available via\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_return_sequences = 1\n",
    "max_length = 100\n",
    "\n",
    "tokens = enc.encode(\"glurp\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "x = tokens.to(\"cuda\")\n",
    "\n",
    "torch.manual_seed(8)\n",
    "torch.cuda.manual_seed(8)\n",
    "\n",
    "while x.size(1) < max_length:\n",
    "    with torch.no_grad():\n",
    "        logits, _ = model(x)\n",
    "        logits = logits[:, -1, :] # last position\n",
    "        probs = func.softmax(logits / 1, dim=-1)\n",
    "        #logits -= (logits / 1).max(1, keepdim=True).values\n",
    "        #logexp = (logits).exp()\n",
    "        #probs = logexp / logexp.sum(dim=0)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        ix = torch.multinomial(topk_probs, 1)\n",
    "        xcol = torch.gather(topk_indices, -1, ix)\n",
    "        x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = x[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "\n",
    "    print(f\"[{i}]\\n{decoded}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "659e70c7-ba13-4db1-91e9-d1392f1710a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m grad_accum_steps \u001b[38;5;241m=\u001b[39m total_batch_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m (partial_batch_size \u001b[38;5;241m*\u001b[39m sequence_length)\n\u001b[1;32m     21\u001b[0m max_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m---> 23\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfigure_optimizers(weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39mmax_lr, device\u001b[38;5;241m=\u001b[39m\u001b[43mdevice\u001b[49m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[1;32m     25\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "max_lr = 3e-4\n",
    "min_lr = 0.1 * max_lr\n",
    "warmup_steps = 10\n",
    "max_steps = 50\n",
    "def get_lr(it):\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it + 1) / warmup_steps\n",
    "    if it > max_steps:\n",
    "        return min_lr\n",
    "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "\n",
    "total_batch_size = 2**19\n",
    "partial_batch_size = 16\n",
    "sequence_length = 1024\n",
    "grad_accum_steps = total_batch_size // (partial_batch_size * sequence_length)\n",
    "\n",
    "max_steps = 50\n",
    "\n",
    "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=max_lr, device=device)\n",
    "for step in range(max_steps):\n",
    "    optimizer.zero_grad()\n",
    "    for partial_step in range(grad_accum_steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            logits, loss = model(x, y)\n",
    "        loss /= grad_accum_steps\n",
    "        loss.backward()\n",
    "    \n",
    "    norm = torch.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "    \n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    print(f\"step {step} | loss: {loss.item():.5f} | norm: {norm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79cd93f-6633-4ab9-b6b9-5a8c40437c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
